\chapter{Marco teórico}
Para el tratamiento de procesos estocásticos y series de tiempo, ha sido mucha la teoría estadística implementada y se ha convertido
en una herramienta importante para tratar observaciones dependientes y entender la naturaleza de dicha dependencia,
la mayoría de estos métodos, asumen que las series de timpo son regulamente espaciadas, sin embargo este escenario
no siempre se da y ahi es donde cobran importancia los procesos estocásticos irregularmente espaciados.

Este capitulo tiene la siguiente estructura, en la seccción 2.1 presentamos la definición matematica de un proceso
estocastico irregularmente espaciado. En la sección 2.2 se presenta un proceso estocástico autorregresivo irregulamente
espaciado. En la sección 2.3 se presenta un proceso estocástico irregularmente espaciado de medias moviles y en la sección
2.3 se presenta un proceso estocástico atorregresivo de medias moviles irregulamente espaciado.


\section{Procesos estocásticos irregularmente espaciados}
Sea $(\Omega,\mathscr{B},P)$ un espacio de probabilidad. Si se define un proceso estocástico como una medida 
que mapea $x: \Omega \mapsto \mathbb{R}^{\mathbb{T}}$, donde 

$$x(\omega)= \lbrace X_{\tau}(\omega), \tau \in \mathbb{T}\rbrace$$

$\mathbb{T}$ es llamado conjunto de índices y la variable aleatoria $X_{\tau}(\omega)$ es llamada coordenada 
del proceso o trayectoria. Consideremos ahora $\mathbb{T}'= \lbrace t_1,t_2, t_3,... \rbrace$, 
con $\Delta_{n+1}=t_{n+1}-t_{n}$, para $n\geq 1$

$$x'= \lbrace X_{\tau}(\omega), \tau \in \mathbb{T}'\rbrace$$

$x'$ es un proceso estocástico irregularmente espaciado. Una serie de tiempo irregularmente espaciada es una 
realización finita de un proceso estocástico irregularmente espaciado. Note que si  $\Delta_{n+1}=t_{n+1}-t_{n}=1$ 
para $n \geq 1$, el proceso $x'$ es un proceso regularmente espaciado, por tanto esta definición es más general.

\section{Formas de describir un proceso estocástico}
Una forma de describir un proceso estocástico $x$, es especificar la función de distribución conjunta de 
$\lbrace X_{\tau_1},X_{\tau_2}, ..., X_{\tau_n}\rbrace$ para todo $n$, este es llamado 
\emph{punto de vista distribucional}. Por otra parte, se puede describir el proceso proporcionando 
una formula para el valor $X_{\tau}$ para cada punto $\tau$ en términos de una familia de variables aleatorias 
con comportamiento probabilístico conocido, esto hace que podamos ver el proceso como una función de otros 
procesos (o cómo familias de procesos iid); esta forma es llamada \emph{punto de vista construccionista}.

\section{El proceso de medias móviles irregular de primer orden (IMA)}
Teniendo en cuenta el conjunto $\mathbb{T}'= \lbrace t_1,t_2, t_3,... \rbrace$ propuesto anteriormente, 
tal que sus diferencias $\Delta_{n+1}$ para $n\geq 1$ están acotados uniformemente lejos de cero. Ahora, 
sea $m: \mathbb{T}' \mapsto \mathbb{R} $ una función tal que $m(t_n)=0$, para cualquier $t_n \in \mathbb{T}'$. 
A continuación, sea $\Gamma: \mathbb{T}'x\mathbb{T}' \mapsto \mathbb{R}$ una función tal que para cualquier 
pareja $t_n,t_s \in \mathbb{T}'$,

\begin{equation*}
\Gamma (t_n,t_s)=
    \begin{cases}
    \gamma_0& |n-s|=0\\
    \gamma_1,\Delta_{max(n,s)} & |n-s|=1\\
    0 & |n-s|\geq1\\
    \end{cases}
\end{equation*}

Note que $\Gamma$ puede ser representada cómo una matriz diagonal así

\begin{equation}
    \Gamma=
	\begin{bmatrix} 
	\gamma_0 & \gamma_1,\Delta_2 & 0 & 0 & ... \\
	\gamma_1,\Delta_2 & \gamma_0 & \gamma_1,\Delta_3 & 0\\
	0 & \gamma_1,\Delta_3 & \gamma_0 & \gamma_1,\Delta_4 \\
	.& & & & . \\
	.& & & & & .\\
	. & & & & & & .
	\end{bmatrix}
	\quad
	\label{mat1}
\end{equation}

Ser $\Gamma_n$ la truncacion $nxn$ de $\Gamma$ y asumiendo $\gamma_1,\Delta_j\neq 0$, para $j=2,...n$, $\Gamma_n$ 
es definida positiva si $\gamma_0$ y $(\frac{\gamma_1,\Delta_{n+1}}{\gamma_0})^2 \leq 1/4$, para $j=2,...,n$

Esto implica que existe un proceso Gaussiano estacionario $\lbrace X_{t_n},t_n, \tau \in \mathbb{T}\rbrace$, 
único hasta la equivalencia, con media 0 y covarianza $\Gamma$. Este proceso es llamado, proceso de medias móviles
 de primer orden irregularmente espaciado de forma general. A continuación se darán las expresiones particulares 
 de este proceso desde los dos puntos de vista antes definidos.

\subsubsection{El punto de vista distribucional}
En (\ref{mat1}), $\gamma_0$ y $\gamma_{1,\Delta_{n+1}}$, para $n\geq1$, representa la varianza y las covarianzas 
de primer orden respectivamente. Definimos la varianza cómo $\gamma_0=\sigma^2(1+\theta^2)$ y las covarianzas de 
primer orden cómo $\gamma_{1,\Delta_{n+1}}=\sigma^2 \theta^{\Delta_{n+1}}$, $\sigma^2 >0$ y $0<\theta <1$. 
Por tanto, obtenemos el proceso estocástico irregularmente espaciado de primer orden con matriz de covarianzas

\begin{equation}
    \Gamma=
	\begin{bmatrix} 
	1+\theta^2 & \theta^{\Delta_2} & 0 & 0 & ... \\
	\theta^{\Delta_2} & 1+\theta^2 & \theta^{\Delta_3}& 0\\
	0 & \theta^{\Delta_3} & 1+\theta^2 & \theta^{\Delta_4} \\
	.& & & & . \\
	.& & & & & .\\
	. & & & & & & .
	\end{bmatrix}
	\quad
	\label{mat2}
\end{equation}

el cual contiene el modelo de medias moviles convencional cómo caso especial. Este es llamado proceso Gaussiano 
irregular de medias moviles de primer orden.

\subsubsection{El punto de vista construccionista}

Ahora, cómo es usual, especificaremos el proceso IMA cómo función de otros procesos estocásticos. 
Sea $\lbrace \epsilon_{t_n}\rbrace n\geq 1$ variables aleatorias independientes que siguen una distribución 
normal $N(0,\sigma^2 c_n(\theta))$ con $\sigma^2 >0, 0<\theta<1, c_1(\theta)=1+\theta^2$ y

$$
c_n(\theta)=1+\theta^2-\frac{\theta^{2\Delta_n}}{c_{n-1}(\theta)} para, n\geq 2
$$

donde $\Delta_n=t_n-t_{n-1}$. El proceso $\lbrace X_{t_n},t_n \in \mathbb{T'} \rbrace$, es decirse tiene un proceso 
IMA si $X_{t_1}=\epsilon_{t_1}$ y para $n\geq 2 $

\begin{equation}
    X_{t_n}= \epsilon_{t_n}+\frac{\theta^{\Delta_n}}{c_{n-1}(\theta)}\epsilon_{t_{n-1}}
\end{equation}
Decimos que $\lbrace X_{t_n},t_n \in \mathbb{T'} \rbrace$ es un proceso IMA con media 
$\mu$ si $\lbrace X_{t_n}-\mu,t_n \in \mathbb{T'} \rbrace$ es un proceso IMA

\section{El proceso Autorregresivo de primer orden irregular (IAR)}

Se denota $X_{t_n}$ a una observación medida en el tiempo $t_n$ y consideramos que $\left\lbrace t_n \right\rbrace$
para $n = 1, ... , n$, se define el proceso IAR de primer orden así:

\begin{equation}
	X_{t_n} = \phi^{t_n - t_{n-1}}X_{t_{n-1}} + \sigma \sqrt{1- \phi^{2(t_n - t_{n-1})} }\varepsilon_{t_n}
\label{eq:IAR}
\end{equation}

Donde $\varepsilon_{t_n}$ son variables aleatorias independientes con media cero
y varianza unitaria, observe que $E(X_{t_n}) = 0$ y $Var(X_{t_n}) = \sigma^2$ para 
todo $X_{t_n}$.\\

Así, la covarianza entre $X_{t_k}$ y $X_{t_j}$ es $E(X_{t_k} ,X_{t_j}) = \sigma^2\phi^{t_k - t_j}$
para $k \geq j$. La matriz de varianzas y autocovarianzas se muestra en  \ref{eq:matIAR}.\\

\begin{equation}
	\begin{bmatrix} 
	1 & \phi^{t_2 - t_1} & \phi^{t_3 - t_1} & .  & . &. &. & \phi^{t_n - t_1}  \\
	& 1 & \phi^{t_3 - t_2} & \phi^{t_4 - t_2}  & . & . & . & \phi^{t_n - t_2} \\
	&  & 1 & \phi^{t_4 - t_3}  & \phi^{t_5 - t_3} & . & . & \phi^{t_n - t_3} \\
	& & & 1& . & . \\
	& & & & .&  & .\\
	& & & & &. & & . \\
	& & & & & & & \phi^{t_n - t_{n-1}} \\
	& & & & & &  & 1 
	\end{bmatrix}
	\quad
	\label{eq:matIAR}
\end{equation}


Así, para cualquier par de tiempos observacionales $s < t$, podemos definir la función
autocovarianza cómo:\\

$\gamma (t-s) = E(X_t, X_s) = \sigma^2 \phi^{t-s}$ \\ 

También, la función de autocorrelación (ACF) está dada por $\rho(t-s) = \frac{\gamma (t-s)}{\gamma (0)} = \phi^{t-s}$\\

La secuencia $X_{t_n}$ corresponde a un proceso debilmente estacionario de segundo orden,
también se puede demostrar que bajo algunas condiciones el proceso es estacionario
y ergódico. Ver \cite[]{eyheramendy2018irregular}.\\

\begin{theorem}
Considere el proceso definido por la ecuación \ref{eq:IAR} y supongamos que el ruido de entrada es una secuencia
iid de variables aleatorias con media cero y varianza unitaria. Además, suponga que $t_j - t_{j-n} \geq C log n$
como $n \to \infty, 0 \ge \phi \ge 1$, donde $C$ es una constante positiva que satisface $Clog \phi^2 \ge -1$. Luego,
existe una solución al proceso definido por la ecuación \ref{eq:IAR} y la secuencia $\left\lbrace X_{t_j} \right\rbrace$
es estacionaria y ergódica.
\end{theorem}


Note también que si $t_n - t_{n-1} = 1$ para todo n, la ecuacion \ref{eq:IAR} se converte en:

\begin{equation}
	X_{t_n} = \phi X_{t_{n-1}} + \sigma \sqrt{1- \phi^2 }\varepsilon_{t_n}
\label{eq:AR1}
\end{equation}

La ecuación \ref{eq:AR1} corresponde al modelo autorregresvo de primer orden para datos regularmente espaciados AR(1). Por lo tanto, el modelo IAR
es una extension del modelo aurorregresivo regular

\subsection{Estimación}
La verosimilitud de una muestra aleatoria ${X_{t_1} ... X_{t_n}}$ puede ser expresada como

$f(X_{t_1} ... X_{t_n}; \theta) = f(X_{t_1};\theta)*f(X_{t_2}|X_{t_1};\theta) * ... * f(X_{t_n}|X_{t_{n-1}};\theta)$ 

donde $\theta = (\sigma^2,\phi)$ es el vector de parámetros del modelo, para poder describir el proceso de estimación, se asume que 
las distribuciones condicionales son Gaussianas, es decir, se asume que 

$f(X_{t_1}) \sim N(0, \sigma^2)$ y $f(X_{t_n}|X_{t_{n-1}};\theta) \sim N(\phi^{t_j-t_{j-1}}X_{t_{j-1}}, \sigma^2(1-\phi^{2(t_j-t_{j-1})}))$ 
para $j = 1,2,3 ... n$

basado en la ecuacion \ref{eq:IAR}, la log-verosimilitud del proceso puede ser escrita como

\begin{equation}
	l(\theta) = \frac{n}{2}log(2\pi) + \frac{1}{2}\sum_{j=1}^{n} log v_{t_j} +  \frac{1}{2}\sum_{j=1}^{n}\frac{e_{t_j}}{v_{t_j}}
	\label{eq:loglik}
\end{equation}

se define $e_{t_1} = X_{t_1}$, $e_{t_j} = X_{t_j}-\phi^{t_j-t_{j-1}}X_{t_{j-1}}$ para $j>1$ y sus varianzas $v_{t_1} = Var(e_{t_1}) $.\\

Observe que el predictor pasado de tiempo finito del proceso en el tiempo $t_j$ es dado por 

$\hat{X}_{t_1} = 0$, y $\hat{X}_{t_j} = \phi^{t_j - t_{j-1}} X_{t_{j-1}}$, para $j = 2, ... , n$ 

Por tanto, $e_{t_j} = X_{t_j} - \hat{X}_{t_j}$ es el error de predicción con varianza $v_{t_1} = Var(e_{t_1} = \sigma^2)$ y
$v_{t_j} = var(e_{t_j}) = \sigma^2[1 - \phi^{2(t_j - t_{j-1})}]$, para $j = 2, ... , n$ 

Por maximización directa de la función de log-verosimilitud (\ref{eq:loglik}), se puede obtener el estimador de máxima verosimilitud de $\sigma^2$

\begin{equation}
	\hat{\sigma}^2 = \frac{1}{n} \sum_{j=1}^{n} \frac{\left(X_{t_j} - \hat{X}_{t_j}\right)}{\tau_{t_j}}, \text{ donde } \tau_{t_j} = v_{t_j}/\sigma^2.
\end{equation}

Pero no es posible encontrar $\hat{\phi}$, el estimador por máxima verosimilitud de $\phi$, por maximización directa de la verosimilitud, es necesario
recurrir a metodos iterativos. 


% \section{Estimación Bootstrap en Procesos Irregulares} \label{section:Bootstrap}

% Para llevar a cabo la inferencia estadística es necesario poder derivar las distribuciones de los estadísticos
% utilizados para la estimación de los parámetros a partir de los datos. Si $\mathrm{N}$ es pequeño,
% o si los parámetros están cerca de los límites, las aproximaciones asintóticas pueden ser bastante pobres \citep{shumway2017time}.
% Además, en el caso de tiempo irregularmente espaciado, las aproximaciones asintóticas necesitan establecer condiciones fuertes
% que son difíciles de cumplir. Para superar estas dificultades y poder obtener aproximaciones de las distribuciones 
% de muestras finitas, podríamos utilizar el método bootstrap. La idea en series temporales es ajustar un modelo adecuado a los datos,
% construir los residuos del modelo ajustado y, a continuación, generar nuevas series incorporando muestras aleatorias de los residuos
% al modelo ajustado. Los residuos suelen ser recientes para que tengan la misma media que las innovaciones del modelo.


% Siguiendo a \cite{bose1990bootstrap}, para $n \geq 2$, define las innovaciones estimadas o residuales de la predicción un paso 
% adelante, como
% $$
% e_{\tau_n}=\sum_{j=0}^{n-1}(-1)^j \frac{\prod_{k=n-j+1}^n \theta^{\Delta_k}}{\prod_{l=n-j}^{n-1} c_l(\theta)} X_{\tau_{n-j}}
% $$
% y $e_{\tau_1}=X_{\tau_1}$. Utilizando la estructura del proceso y asumiendo que el modelo ajustado es, de hecho, el modelo verdadero
%  para los datos tenemos:
% $$
% e_{\tau_n}=\varepsilon_{\tau_n}+(-1)^{n-1} \frac{\prod_{k=1}^n \theta^{\Delta_k}}{\prod_{l=0}^{n-1} c_l(\theta)} \varepsilon_{t_0} .
% $$

% Por lo tanto, $e_{\tau_n}$ y $\varepsilon_{\tau_n}$ son cercanos para todo $n$ grande si $|\theta|<1$, lo que demuestra que el remuestreo es adecuado en esta situación.


\section{Modelo autoregresivo complejo irregular (CIAR)}

Para derivar una extensión compleja del modelo \ref{eq:IAR}, se sigue el mismo proceso que para series de tiempo regulares. Suponiendo que $x_{t_{j}}$ es una secuencia de valores complejos, 
tal que $x_{t_{j}}=y_{t_{j}}+i z_{t_{j}} \text{ } \forall j=1, \ldots, n$, y de manera similar, $\phi=\phi^{R}+i \phi^{I}$ es el coeficiente complejo del modelo 
y $\varepsilon_{t_{j}}=\varepsilon_{t_{j}}^{R}+i \varepsilon_{t_{j}}^{I}$ es un ruido blanco complejo, definimos el proceso CIAR como

\begin{equation}
	y_{t_{j}}+i z_{t_{j}}=\left(\phi^{R}+i \phi^{I}\right)^{t_{j}-t_{j-1}}\left(y_{t_{j-1}}+i z_{t_{j-1}}\right)+\sigma_{t_{j}}\left(\varepsilon_{t_{j}}^{R}+i \varepsilon_{t_{j}}^{I}\right)
	\label{eq:CIAR}
\end{equation}

donde $\sigma_{t_{j}}=\sigma \sqrt{1-\left|\phi^{t_{j}-t_{j-1}}\right|^{2}}$ y $|$.$|$ es el módulo de un número complejo. 
Suponemos que solo se observa la parte real $y_{t_{j}}$ y que la parte imaginaria $z_{t_{j}}$ es un proceso latente. Además, se asume que $\varepsilon_{t_{j}}^{R}$ y $\varepsilon_{t_{j}}^{I}$, 
la parte real e imaginaria de $\varepsilon_{t_{j}}$, respectivamente, son independientes con media cero y varianza positiva $\mathbb{V}\left(\varepsilon_{t}^{R}\right)=1$ y $\mathbb{V}\left(\varepsilon_{t}^{I}\right)=c$, 
respectivamente, donde $c$ es un parámetro fijo que toma valores en $\mathbb{R}^{+}$. Generalmente, se supone que $c=1$, y los valores iniciales son $y_{t_{1}}=\sigma \varepsilon_{t_{1}}^{R}$ y $z_{t_{1}}=\sigma \varepsilon_{t_{1}}^{I}$.
En el siguiente lema, se presentan algunas de las propiedades de este proceso.

\begin{lemma}
	Considere el proceso CIAR $x_{t_{j}}$ descrito por la ecuación \ref{eq:CIAR}. Defina $\gamma_{0}=\mathbb{E}\left(\bar{x}_{t{j}} x_{t_{j}}\right), \gamma_{k}=\mathbb{E}\left(\bar{x}_{t{j+k}} x_{t_{j}}\right)$ 
	y $\rho_{k}$ como la varianza, autocovarianza y autocorrelación, respectivamente, del proceso $x_{t_{j}}$. A continuación, el valor esperado, la varianza, la autocovarianza y la autocorrelación del proceso 
	\begin{itemize}
		\item $\mathbb{E}\left(x_{t_{j}}\right) = 0$
		\item $\mathbb{V}\left(x_{t_{j}}\right)=\gamma_{0}=\mathbb{E}\left(\bar{x}_{t{j}} x_{t_{j}}\right)=\sigma^{2}(1+c)$,
		\item $\gamma_{k}=\mathbb{E}\left(\bar{x}_{t{j+k}} x_{t_{j}}\right)=\overline{\phi^{\Delta_{k}}} \sigma^{2}(1+c)$,
		\item $\rho_{k}=\overline{\phi^{\Delta_{k}}}$
	\end{itemize}
	donde $\Delta_{k}=t_{j+k}-t_{j}$ denota las diferencias de tiempo entre los tiempos de observación $t_{j+k}$ y $t_{j}$. Además, 
	$\bar{x}{t{j}}$ es el conjugado complejo de $x_{t_{j}}$.
	\label{Lemma1}
\end{lemma}
 
 Si $|\phi|=\left|\phi^{R}+i \phi^{I}\right|<1$, los resultados en el Lema \ref{Lemma1} muestran que la secuencia compleja $x_{t_{j}}$ es un proceso débilmente estacionario. 
 Observamos que la ACF $\rho_{k}$ del proceso CIAR decae a una tasa $\phi^{t_{j+k}-t_{j}}$ (la llamada decaimiento exponencial). Esta estructura de autocorrelación es diferente a la de un proceso CARFIMA de memoria antipersistente o de memoria intermedia, ya que la ACF de estos últimos decae más lentamente que un decaimiento exponencial. Por lo tanto, aunque ambos modelos pueden ajustar series de tiempo irregulares con valores negativos de la ACF, el uso apropiado de estos modelos dependerá de la estructura de correlación de los datos. Para tomar una decisión sobre el modelo más adecuado para los datos, se pueden utilizar técnicas basadas en la verosimilitud de los datos, como AIC, BIC, y así sucesivamente.